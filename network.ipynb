{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c9605b-b40b-4bd8-ad8b-f0be943a8439",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "## Programming a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a6a5c9-67c3-478c-b390-a00fad92cf15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb64dbb-ce83-4edf-89ad-79a49e239ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReLULayer(object):\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the input \n",
    "        relu = ... # your code here \n",
    "        return relu\n",
    "        \n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input \n",
    "        downstream_gradient = ... # your code here\n",
    "        return downstream_gradient\n",
    "    \n",
    "    def update(self, learning_rate): \n",
    "        pass # ReLU is parameter -free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605802b1-b2fc-41c9-8733-c995a6414aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation \n",
    "        self.input = input\n",
    "        # return the softmax of the input\n",
    "        softmax = ... # your code here\n",
    "        return softmax\n",
    "    \n",
    "    def backward(self, predicted_posteriors, true_labels):\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # (use cross -entropy loss and the chain rule for softmax ,\n",
    "        # as derived in the lecture)\n",
    "        downstream_gradient = ... # your code here \n",
    "        return downstream_gradient\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        pass # softmax is parameter -free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2312a429-8fd8-4bde-a7ee-65083ac5f760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearLayer(object):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # randomly initialize weights and intercepts \n",
    "        self.B = np.random.normal(...) # your code here \n",
    "        self.b = np.random.normal(...) # your code here\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # compute the scalar product of input and weights\n",
    "        # (these are the preactivations for the subsequent non-linear layer)\n",
    "        preactivations = ... # your code here\n",
    "        return preactivations\n",
    "    \n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of the weights from\n",
    "        # upstream_gradient and the stored input\n",
    "        self.grad_b = ... # your code here\n",
    "        self.grad_B = ... # your code here\n",
    "        # compute the downstream gradient to be passed to the preceding layer \n",
    "        downstream_gradient = ... # your code here\n",
    "        return downstream_gradient\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        # update the weights by batch gradient descent \n",
    "        self.B = self.B - learning_rate * self.grad_B \n",
    "        self.b = self.b - learning_rate * self.grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9983ea73-0890-46b6-aef4-0965dfac78e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        # constuct a multi -layer perceptron\n",
    "        # with ReLU activation in the hidden layers and softmax output\n",
    "        # (i.e. it predicts the posterior probability of a classification problem) #\n",
    "        # n_features: number of inputs\n",
    "        # len(layer_size): number of layers\n",
    "        # layer_size[k]: number of neurons in layer k\n",
    "        # (specifically: layer_sizes[-1] is the number of classes)\n",
    "        \n",
    "        self.n_layers = len(layer_sizes) \n",
    "        self.layers = []\n",
    "        \n",
    "        # create interior layers (linear + ReLU)\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append(LinearLayer(n_in, n_out)) \n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "            \n",
    "        # create last linear layer + output layer\n",
    "        n_out = layer_sizes[-1] \n",
    "        self.layers.append(LinearLayer(n_in, n_out)) \n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "   \n",
    "    def forward(self, X):\n",
    "        # X is a mini-batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X (in case instances are images)\n",
    "        X = X.reshape(batch_size , -1)\n",
    "        \n",
    "        # compute the forward pass\n",
    "        # (implicitly stores internal activations for later backpropagation) \n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result) \n",
    "            return result\n",
    "    \n",
    "    def backward(self, predicted_posteriors, true_classes):\n",
    "        # perform backpropagation w.r.t. the prediction for the latest mini-batch X \n",
    "        ... # your code here\n",
    "        \n",
    "    def update(self, X, Y, learning_rate): \n",
    "        posteriors = self.forward(X) \n",
    "        self.backward(posteriors , Y)\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "        \n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size \n",
    "        for i in range(n_epochs):\n",
    "            # print(\"Epoch\", i)\n",
    "            # reorder data for every epoch\n",
    "            # (i.e. sample mini-batches without replacement) \n",
    "            permutation = np.random.permutation(N)\n",
    "            \n",
    "            for batch in range(n_batches):\n",
    "                # create mini-batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]] \n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "                \n",
    "                # perform one forward and backward pass and update network parameters\n",
    "                self.update(x_batch , y_batch, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859dab85-1256-4f0e-b133-34fadc152316",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# create network\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m network \u001b[38;5;241m=\u001b[39m \u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m     28\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain(X_train , Y_train , n_epochs , batch_size , learning_rate)\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mMLP.__init__\u001b[0;34m(self, n_features, layer_sizes)\u001b[0m\n\u001b[1;32m     15\u001b[0m n_in \u001b[38;5;241m=\u001b[39m n_features\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_out \u001b[38;5;129;01min\u001b[39;00m layer_sizes[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mLinearLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_out\u001b[49m\u001b[43m)\u001b[49m) \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(ReLULayer())\n\u001b[1;32m     19\u001b[0m     n_in \u001b[38;5;241m=\u001b[39m n_out\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mLinearLayer.__init__\u001b[0;34m(self, n_inputs, n_outputs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs \u001b[38;5;241m=\u001b[39m n_outputs\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# randomly initialize weights and intercepts \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# your code here \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n",
      "File \u001b[0;32mmtrand.pyx:1540\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.normal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:585\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'ellipsis'"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    # set training/test set size\n",
    "    N = 2000\n",
    "    \n",
    "    # create training and test data\n",
    "    X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
    "    X_test,  Y_test  = datasets.make_moons(N, noise=0.05)\n",
    "    n_features = 2\n",
    "    n_classes = 2\n",
    "    \n",
    "    # standardize features to be in [-1, 1]\n",
    "    offset  = X_train.min(axis=0) \n",
    "    scaling = X_train.max(axis=0) - offset\n",
    "    X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "    X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "\n",
    "    # set hyperparameters (play with these!)\n",
    "    layer_sizes = [5, 5, n_classes] \n",
    "    n_epochs = 5\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.05\n",
    "    \n",
    "    # create network\n",
    "    network = MLP(n_features, layer_sizes)\n",
    "    \n",
    "    # train\n",
    "    network.train(X_train , Y_train , n_epochs , batch_size , learning_rate)\n",
    "    \n",
    "    # test\n",
    "    predicted_posteriors = network.forward(X_test)\n",
    "    \n",
    "    # determine class predictions from posteriors by winner -takes -all rule\n",
    "    predicted_classes = ... # your code here\n",
    "    # compute and output the error rate of predicted_classes\n",
    "    error_rate = ... # your code here\n",
    "    print(\"error rate:\", error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378cec0-9817-41ad-9e2c-8d18f17db2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
