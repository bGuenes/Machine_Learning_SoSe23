{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998a7000-c789-48a9-89cb-4e98f127be86",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "## Programming a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901158fa-7595-4ed5-bb4b-133545e507be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d70185-e4df-4190-a5df-f1c0c0a69aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer(object):\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the input\n",
    "        relu = self.input * (self.input > 0).astype(float) # give the input value, if it's bigger than 0\n",
    "        return relu\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input\n",
    "        downstream_gradient = upstream_gradient * (self.input > 0).astype(float) # upstream gradient with the derivative of ReLu at the input\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # ReLU is parameter-free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0619d75-8ca2-4f70-a0a9-9f00e2eac469",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Works the same as the sample solution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2d459cf-9f43-4a38-98c1-b58e0b1dcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the softmax of the input\n",
    "        softmax = np.exp(self.input) / (np.exp(self.input)).sum()\n",
    "        return softmax\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_labels):\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # (use cross-entropy loss and the chain rule for softmax,\n",
    "        #  as derived in the lecture)\n",
    "        downstream_gradient = (1/len(predicted_posteriors)) * (predicted_posteriors - np.eye(self.n_classes)[true_labels])\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # softmax is parameter-free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a031c3b-43a6-4618-a724-4da33138791d",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">In the exponent of the softmax we forgot to substact the max element of the input. <br> The downstream gradient is the same as the sample solution, but calculated a bit differently. The result shouldn't be affected by that.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae7e72e5-3a6d-4cc6-9987-e7ee386d3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(object):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs  = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # randomly initialize weights and intercepts\n",
    "        self.B = np.random.normal(0, 1/np.sqrt(self.n_inputs), size = (n_inputs, self.n_outputs))\n",
    "        self.b = np.random.normal(0, 1/np.sqrt(self.n_inputs), size = (self.n_outputs,))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # compute the scalar product of input and weights\n",
    "        # (these are the preactivations for the subsequent non-linear layer)\n",
    "        #print(\"shape input: \", self.input.shape)\n",
    "        preactivations = np.dot(self.input, self.B) + self.b\n",
    "        return preactivations\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of the weights from\n",
    "        # upstream_gradient and the stored input\n",
    "        self.grad_b = np.sum(upstream_gradient, axis = 0)\n",
    "        self.grad_B = np.dot(self.input.T, upstream_gradient)\n",
    "        # compute the downstream gradient to be passed to the preceding layer\n",
    "        downstream_gradient = np.dot(upstream_gradient, self.B.T)\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # update the weights by batch gradient descent\n",
    "        self.B = self.B - learning_rate * self.grad_B\n",
    "        self.b = self.b - learning_rate * self.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e65f9-b34f-4080-bf59-da9e7862ac91",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">works as expected</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c830665-b680-4a2a-ac5c-9ed962dcacf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        # constuct a multi-layer perceptron\n",
    "        # with ReLU activation in the hidden layers and softmax output\n",
    "        # (i.e. it predicts the posterior probability of a classification problem)\n",
    "        #\n",
    "        # n_features: number of inputs\n",
    "        # len(layer_size): number of layers\n",
    "        # layer_size[k]: number of neurons in layer k\n",
    "        # (specifically: layer_sizes[-1] is the number of classes)\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers   = []\n",
    "\n",
    "        # create interior layers (linear + ReLU)\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append(LinearLayer(n_in, n_out))\n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "\n",
    "        # create last linear layer + output layer\n",
    "        n_out = layer_sizes[-1]\n",
    "        self.layers.append(LinearLayer(n_in, n_out))\n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is a mini-batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X (in case instances are images)\n",
    "        X = X.reshape(batch_size, -1)\n",
    "\n",
    "        # compute the forward pass\n",
    "        # (implicitly stores internal activations for later backpropagation)\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_classes):\n",
    "        # perform backpropagation w.r.t. the prediction for the latest mini-batch X\n",
    "        # Compute the loss derivative with respect to the predicted posteriors\n",
    "        upstream_gradient = self.layers[-1].backward(predicted_posteriors, true_classes)\n",
    "\n",
    "        # Backpropagate the gradients through the layers\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            upstream_gradient = layer.backward(upstream_gradient)\n",
    "\n",
    "\n",
    "    def update(self, X, Y, learning_rate):\n",
    "        posteriors = self.forward(X)\n",
    "        self.backward(posteriors, Y)\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range(n_epochs):\n",
    "            # print(\"Epoch\", i)\n",
    "            # reorder data for every epoch\n",
    "            # (i.e. sample mini-batches without replacement)\n",
    "            permutation = np.random.permutation(N)\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "                # create mini-batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]]\n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "\n",
    "                # perform one forward and backward pass and update network parameters\n",
    "                self.update(x_batch, y_batch, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c895f1-42c4-4a7d-9a5f-f1f52300dbe9",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Basically same as sample solution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "581efe2f-9aca-452c-b360-5bcebaea8afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate: 0.361\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    # set training/test set size\n",
    "    N = 2000\n",
    "\n",
    "    # create training and test data\n",
    "    X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
    "    X_test,  Y_test  = datasets.make_moons(N, noise=0.05)\n",
    "    n_features = 2\n",
    "    n_classes  = 2\n",
    "\n",
    "    # standardize features to be in [-1, 1]\n",
    "    offset  = X_train.min(axis=0)\n",
    "    scaling = X_train.max(axis=0) - offset\n",
    "    X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "    X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "\n",
    "    # set hyperparameters (play with these!)\n",
    "    layer_sizes = [5,5, n_classes]\n",
    "    n_epochs = 5\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.05\n",
    "\n",
    "    # create network\n",
    "    network = MLP(n_features, layer_sizes)\n",
    "\n",
    "    # train\n",
    "    network.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "\n",
    "    # test\n",
    "    predicted_posteriors = network.forward(X_test)\n",
    "    # determine class predictions from posteriors by winner-takes-all rule\n",
    "    predicted_classes = np.argmax(predicted_posteriors, axis=1)\n",
    "    # compute and output the error rate of predicted_classes\n",
    "    error_rate = 1- np.mean(predicted_classes == Y_test)\n",
    "    print(\"error rate:\", error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc97fe6-5b64-4fbf-b2f8-10a36c86dab9",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">Same as sample solution</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a5b99-efa5-4d1c-ae15-3f6e96889a91",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight: bold\">The reason our code didn't work in the first hand in, was because we had an error in the softmax function. With the corrected function for the softmax the NN works fine. <br>\n",
    "Because our code didn't work, it wasn't possible to compare the different layer sizes. This could be easily done by changing the parameters in layer_sizes.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
